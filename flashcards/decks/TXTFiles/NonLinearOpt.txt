#separator:tab
#html:true
#tags column:3
<b>Thrm. 3.48 a,&nbsp;</b>Prove the characterization of convexity in&nbsp;\(C^1\)&nbsp;(ONLY FIRST PART NEEDED FOR EXAM)	Definition:<br>Let&nbsp;\(f\in C^1\)&nbsp;on an open convex set&nbsp;\(S\).<br>\(f\)&nbsp;is convex on&nbsp;\(S \iff f(\mathbf{ x}) \geq f(\mathbf{\bar x}) + \nabla f(\mathbf{\bar x})^T(\mathbf x - \mathbf{\bar x}), \forall \mathbf x,\mathbf{\bar x} \in S.\)<ol><li>\(f\)&nbsp;is convex on&nbsp;\(S \iff f(\mathbf{ x}) \geq f(\mathbf{\bar x}) + \nabla f(\mathbf{\bar x})^T (\mathbf x - \mathbf{\bar x}), \forall \mathbf x,\mathbf{\bar x} \in S.\)</li><li>\(f\)&nbsp;is convex on&nbsp;\(S \iff \left[\nabla f(\mathbf{x}) -  \nabla f(\mathbf{\bar x})\right]^T(\mathbf{ x} -\mathbf {\bar x} )  \geq 0,\forall \mathbf x, \mathbf{\bar x} \in S.\)</li></ol>To prove the statements both 1 and 2 need their respective implication proved&nbsp;\(\Longleftarrow\)&nbsp;and&nbsp;\(\Longrightarrow\).<br>1.<br>\('\Longrightarrow' \text{part}:\)<br>Assume left side,&nbsp;\(f\)&nbsp;is convex on&nbsp;\(S\).<br>Let&nbsp;\(\mathbf x^1, \mathbf x^2\in S, \lambda \in (0,1)\).<br>By convexity&nbsp;\(f(\lambda\mathbf x^1 + (1-\lambda)\mathbf x^2) \leq \lambda f(\mathbf x^1) + (1-\lambda)f(\mathbf x^2)\)<br>\(f(\mathbf x^1) - f(\mathbf x^2) \geq \frac{1}{\lambda}\left(f(\lambda\mathbf x^1 + (1-\lambda)\mathbf x^2) - f(\mathbf x^2)\right)\)&nbsp;(this works because&nbsp;\(\frac{1-\lambda}{\lambda}f(\mathbf x^2) =\frac{1}{\lambda}f(\mathbf x^2) - \frac{\lambda}{\lambda}f(\mathbf x^2)\))<br>Right hand side of the inequality tends to the directional derivative of&nbsp;\(f\)&nbsp;at&nbsp;\(\mathbf x^2\)&nbsp;in the direction&nbsp;\((\mathbf x^1 - \mathbf x^2) \)&nbsp;when&nbsp;\(\lambda \downarrow 0\), so the limit becomes<br>\(f(\mathbf x^1) - f(\mathbf x^2) \geq \nabla f(\mathbf x^2)(\mathbf x^1-\mathbf x^2) \implies\)\(f(\mathbf x) \geq f(\mathbf {\bar x}) + \nabla f(\mathbf{\bar x})^T(\mathbf x -\mathbf {\bar x} )\)<br>\('\Longleftarrow' \text{part}:\)<br>We have that<br>\(f(\mathbf x^1) \geq f(\lambda\mathbf x^1 + (1-\lambda)\mathbf x^2) + (1-\lambda)\nabla f(\lambda \mathbf x^1 + (1-\lambda)\mathbf x^2)^T(\mathbf x^1 -\mathbf x^2)\),<br>\(f(\mathbf x^2) \geq f(\lambda\mathbf x^1 + (1-\lambda)\mathbf x^2) + \lambda\nabla f(\lambda \mathbf x^1 + (1-\lambda)\mathbf x^2)^T(\mathbf x^2 -\mathbf x^1)\).<br>Respectively multiply both inequalities by&nbsp;\(\lambda\)&nbsp; and&nbsp;\((1-\lambda)\)&nbsp;to provide the sought result.<br><br>2.<br>\('\Longrightarrow' \text{part}:\)<br>Using 1 and the two inequalities:<br>\(f(\mathbf{\bar x}) \geq f(\mathbf{ x}) + \nabla f(\mathbf{x})^T(\mathbf{\bar x} - \mathbf{x}), \mathbf{\bar x}, \mathbf{ x}\in S\),<br>\(f(\mathbf{ x}) \geq f(\mathbf{ \bar x}) + \nabla f(\mathbf{\bar x})^T(\mathbf{ x} - \mathbf{\bar x}), \mathbf{\bar x}, \mathbf{ x}\in S\),<br>Added together yields&nbsp;<br>\(\left[ \nabla f(\mathbf{x}) - \nabla f(\mathbf{\bar x})  \right](\mathbf{x} - \mathbf{\bar x}) \geq 0, \forall \mathbf{\bar x}, \mathbf{x} \in S.\)<br><br>\('\Longleftarrow' \text{part}:\)<br>The MVT. states that<br>\(f(\mathbf x^2) - f(\mathbf x^1) = \nabla f(\mathbf x)^T(\mathbf x^2 - \mathbf x^1)\), where&nbsp;\(\mathbf x = \lambda \mathbf x^1 + (1-\lambda)\mathbf x^2, \lambda \in (0,1)\).<br>By assumption&nbsp;\(\left[ \nabla f(\mathbf{x}) - \nabla f(\mathbf{ x}^1)  \right](\mathbf{x} - \mathbf{ x}^1) \geq 0\), so&nbsp;\((1- \lambda)\left[ \nabla f(\mathbf{x}) - \nabla f(\mathbf{x}^1)  \right](\mathbf{x^2} - \mathbf{x}^1) \geq 0\).<br>From this follows that&nbsp;\(\nabla f(\mathbf{x})^T(\mathbf x^2 -\mathbf x^1)\geq \nabla f(\mathbf x ^1)^T(\mathbf x ^2 - \mathbf x ^1).\)<br>By using the previous inequality together with the mean value theorem we get&nbsp;\(f(\mathbf x ^2)\geq f(\mathbf x ^1) + \nabla f(\mathbf x ^1)^T(\mathbf x ^2 - \mathbf x ^1)\)&nbsp;\(\blacksquare\)	
<b>Thrm. 4.3,&nbsp;</b>Fundamental Theorem of global optimality	<b>Definition</b><br>The minimization problem&nbsp;<br>\(\min f(x), \text{ subject to } x\in S\)&nbsp;where&nbsp;\(S \subseteq \mathbb R^n\not \;\;\emptyset\),<br>\(f: \mathbb R^n \to \mathbb R\cup\{+\infty\}\)&nbsp;where&nbsp;\(S\)&nbsp;is convex and&nbsp;\(f\)&nbsp;is a convex function on&nbsp;\(S\)&nbsp;implies that all local minimum of&nbsp;\(f\)&nbsp;over&nbsp;\(S\)&nbsp;is also a global minimum.<br><br><b>Proof</b><br>Assume&nbsp;\(x^*\in S\)&nbsp;is a local min but not a global min.&nbsp;<br>let&nbsp;\(\bar x\in S\)&nbsp;be a point s.t.&nbsp;\(f(\bar x) \lt f(x^*).\)<br>For any&nbsp;\(\lambda\in(0,1),\)&nbsp;\(\lambda(\bar x) + (1-\lambda)(x^*)\in S\)&nbsp;by the definition of convexity.<br>Since&nbsp;\(f\)&nbsp;is convex<br>\(f\bigr(\lambda(\bar x) + (1-\lambda)(x^*)\bigr)\in S \leq \lambda f(\bar x) + (1-\lambda)f(x^*)\in S \implies\)<br>\(f\bigr(\lambda(\bar x) + (1-\lambda)(x^*)\bigr)\in S \lt \lambda f(x^*) + (1-\lambda)f(x^*)\in S \)<br>If&nbsp;\(\lambda \downarrow 0\)&nbsp;left hand side goes to&nbsp;\(f(x^*),\)&nbsp;implying that&nbsp;\(f(x^*) &gt; f(x^*)\), reductio ad absurdum&nbsp;\(\square\)	
<b>Thrm. 5.29,&nbsp;</b>State and prove the KKT optimality conditions	"<b>Definition</b><br>Assume that for a given point&nbsp;\(\mathbf x^*\in S\)&nbsp;Abadie's CQ hold. if&nbsp;\(\mathbf x\in S\)&nbsp;is a local minimum of&nbsp;\(f\)&nbsp;over&nbsp;\(S\)&nbsp;then there exists a vector&nbsp;\(\boldsymbol \mu\)&nbsp;such that<br><ol><li>\(\nabla f(\mathbf x^*) + \sum_i^m\mu_i^*\nabla g_i(\mathbf x^*)   = \mathbf 0^n\)&nbsp;(No feasible descent direction)<br></li><li>\(\mu_i g_i(\mathbf x^*)=  0\;\; \forall_i\)&nbsp;(Complementary slackness)</li><li>\(\boldsymbol \mu\geq \mathbf 0^m\)&nbsp;(Dual feasibility)</li></ol><div><br></div><div>That is<br>\(\left. \begin{array}{ll}
\mathbf x^* \; \text{local minimum of f over S }\\
\text{Abadie's CQ holds at }\mathbf x^* 
\end{array}\right\}\implies \exists \boldsymbol \mu\in \mathbb R^m, \text{ above holds}\).<br></div><div><br></div><div><b>Proof</b><br>Given geometric optimality&nbsp;\(\mathring{F}(\mathbf x^*) \cap T_s(\mathbf x^*) = \emptyset\),&nbsp;it can be assumed that&nbsp;\(\mathring{F}(\mathbf x^*)\cap G(\mathbf x^*) = \emptyset\)&nbsp;(Abadie's hold).&nbsp;</div><div><br></div><div><br>Construct a matrix&nbsp;\( A\)&nbsp;with columns&nbsp;\(\nabla g_i(\mathbf x^*), i\in \mathcal I(\mathbf x ^*)\). Then the system&nbsp;\( A^T\mathbf p\leq \mathbf 0^{|\mathcal{I} (\mathbf x^*)|}\)&nbsp;and&nbsp;\(-\nabla f(\mathbf x^*)^T\mathbf p &gt; 0\)&nbsp;has no solutions. By Farkas' lemma the system&nbsp;\( A\mathbf  \xi = -\nabla f(\mathbf x^*), \boldsymbol \xi\geq\mathbf 0^{|\mathcal{I(\mathbf x^*)}|}\)&nbsp;has a solution. Define the vector&nbsp;</div>\(\boldsymbol \mu_{\mathcal{I}(\mathbf x^*)}=\boldsymbol \xi\)&nbsp;and&nbsp;\(\mu_i= 0\)&nbsp;when&nbsp;\(i \notin \mathcal I(\mathbf x^*)\). Then, the so defined&nbsp;\(\boldsymbol \mu \)&nbsp;verifies the KKT conditions."	
<b>Thrm. 4.22,&nbsp;</b>Necessary optimality conditions,&nbsp;\(C^1\)&nbsp;case	<b>Definition</b><br>Suppose&nbsp;\(S \subseteq \mathbb R^n\)&nbsp;and&nbsp;\(f: \mathbb R^n \to \mathbb R\cup\{+\infty\}\)&nbsp;is in&nbsp;\(C^1\)&nbsp;around&nbsp;\(\mathbf x\in S\)&nbsp;for&nbsp;\(f(\mathbf x)\lt +\infty\). Prove that<br><ol><li>If&nbsp;\(\mathbf x^* \in S\)&nbsp;is a local minimum of&nbsp;\(f\)&nbsp;over&nbsp;\(S\)&nbsp;then&nbsp;\(\nabla f(\mathbf x^*)^T\mathbf p \geq 0\)&nbsp;holds for every feasible direction&nbsp;\(\mathbf p\)&nbsp;from&nbsp;\(\mathbf x^*\).</li><li>Suppose that&nbsp;\(S\)&nbsp;is convex and that&nbsp;\(f\)&nbsp;is convex on&nbsp;\(S\)&nbsp;in&nbsp;\(C^1\), then if&nbsp;\(\mathbf x^*\in S\)&nbsp;is a local minimum of&nbsp;\(f\)&nbsp;over&nbsp;\(S\)&nbsp;then&nbsp;\(\nabla f(\mathbf x^*)^T(\mathbf x - \mathbf x^*)\geq0,\;\forall\mathbf x\in S\)&nbsp;holds.<br></li></ol><div><b>Proof</b></div><ol><li>Taylor expand around&nbsp;\(\mathbf x^*\),&nbsp;\(f(\mathbf x^* + \alpha \mathbf p) = f(\mathbf x^*) + \alpha\nabla f(\mathbf x^*)^T\mathbf p + o(a), a\in(0,\delta], \delta\gt 0.\)<br>If there exists a direction&nbsp;\(\mathbf p\)&nbsp;for which it holds that&nbsp;\(\nabla f(\mathbf x^*)^T\mathbf p\lt 0\), then&nbsp;\(f(\mathbf x^* + \alpha \mathbf p)\lt f(\mathbf x^*)\)&nbsp;for small enough&nbsp;\(\alpha\). If&nbsp;\(\mathbf p\)&nbsp;is a feasible direction, then a contradiction is fulfilled.<br><br></li><li>If&nbsp;\(S\)&nbsp;is convex then for every&nbsp;\(\mathbf x\in S, \mathbf p := \mathbf x - \mathbf x^*\)&nbsp;is a feasible direction.&nbsp;\(\nabla f(\mathbf x^*)(\mathbf x - \mathbf x^*)\geq 0\)&nbsp;follows from proof 1.</li></ol>&nbsp;<br><div><br></div>	
<b>Thrm. 8.10&nbsp;</b>Existence and properties of optimal solutions (Linear Programming Models)	<b>Definition<br></b>Let&nbsp;\(V  \)&nbsp;be the extreme points&nbsp;\(V: =\{\mathbf v^1, \ldots , \mathbf v^k\}\)&nbsp;to&nbsp;the standard form polyhedron&nbsp;\(P :=\{\mathbf x \in \mathbb R^n | A\mathbf x = \mathbf b, \mathbf x \geq \mathbf 0 \} \),&nbsp;\(D : = \{\mathbf d^1, \ldots ,\mathbf d^n\}\)&nbsp;be the extreme directions for&nbsp;\(C : = \{\mathbf x\in \mathbb R^n| A\mathbf x = \mathbf 0, \; \mathbf x\geq \mathbf 0\}\). Consider the linear program&nbsp;<br>\(\min z = \mathbf c^T\mathbf x,\)<br>\(\mathbf x \in P\).<br><br>Then it holds that<br><ol><li>The problem has a finite optimal solution if and only if&nbsp;\(P\)&nbsp;is nonempty and&nbsp;\(z\)&nbsp;is lower bounded on&nbsp;\(P\), that is,&nbsp;\(P\)&nbsp;is nonempty and&nbsp;\(\mathbf c^T\mathbf d^j\geq 0,\;\)\(\forall \mathbf d^j\in D\).</li><li>If the problem has a finite optimal solution, then a optimal solution is found among the extreme points.</li></ol><div><b>Proof</b></div><ol><li>Let&nbsp;\(\mathbf x \in P\). The it follows that&nbsp;\(\mathbf x = \sum_{i}^k\alpha_i \mathbf v^i + \sum_{j}^r\beta_j\mathbf d^j\),&nbsp;for some&nbsp;\(\alpha_1, \ldots, \alpha_k \geq 0\)&nbsp;s.t.&nbsp;\(\sum \alpha =1\)&nbsp;and&nbsp;\(\beta_1\ldots\beta_r \geq 0\). Hence the multiplication of&nbsp;\(\mathbf c^T\)&nbsp;provides that&nbsp;\(\mathbf c^T\mathbf x = \sum_{i}^k\alpha_i \mathbf c^T\mathbf v^i + \sum_{j}^r\beta_j \mathbf c^T\mathbf d^j \). Given that&nbsp;\(z = \mathbf c^T \mathbf x\)&nbsp;it is clear it is dependent on the variations of the weights&nbsp;\(\alpha_i\)&nbsp;and&nbsp;\(\beta_j\), who are bounded by&nbsp;\(\sum \alpha =1\)&nbsp;and the fact that&nbsp;\(\beta_j\mathbf d^j \geq 0 \;\; \forall \mathbf d^j\in D\), since otherwise the corresponding&nbsp;\(\beta\)&nbsp;to the inequality would tend to infinity as it provides that&nbsp;\(z \to -\infty\). <br>Given that&nbsp;\(\mathbf c^T\mathbf d^j\geq 0\;\forall j\)&nbsp;the optimal value is to set&nbsp;\(\boldsymbol \beta = \mathbf 0\), removing any addition to the minimization of&nbsp;\(\mathbf c^T\mathbf x\). This makes&nbsp;the problem only to minimize&nbsp;\(\mathbf c^T\mathbf x = \sum_{i}^k\alpha_i \mathbf c^T\mathbf v^i\), that is, in the convex hull of&nbsp;\(P\).<br><br></li><li>Assume&nbsp;\(\mathbf x\in P\)&nbsp;is an optimal solution and let&nbsp;\(\mathbf x\)&nbsp;be expressed as the summation of&nbsp;\(\alpha \)&nbsp;and&nbsp;\(\beta\). Setting&nbsp;\(\forall \beta = 0\)&nbsp;again we have that&nbsp;\(\mathbf x= \sum^k_i\alpha_i\mathbf v^i \). <br>Further, let&nbsp;\(a\in \arg \min_{i\in\{1,\ldots,k\}}\mathbf c^T\mathbf v^i\).Then it holds that&nbsp;<br>\(\mathbf c^T \mathbf v^a = \mathbf c^T\mathbf v^a\sum_i^k\alpha_i =  \sum_i^k\alpha_i \mathbf c^T\mathbf v^a\leq \sum_i^k\alpha_i\mathbf c^T\mathbf v^i = \mathbf c^T\mathbf x\).<br>That is, the extreme point&nbsp;\(\mathbf v^a\)&nbsp;is a global minimum.</li></ol>	
<b>Thrm. 4.23,</b>&nbsp;Necessary and sufficient global optimality conditions	"<b>Definition</b><br>Suppose&nbsp;\(S\subseteq \mathbb R^n\not\;\;\emptyset\)&nbsp;and convex. Let&nbsp;\(f:\mathbb R^n \to \mathbb R\)&nbsp;be convex and&nbsp;\(C^1\)&nbsp;on&nbsp;\(S\).&nbsp;<br>Prove that this implies that&nbsp;<br>\(\mathbf x^*\)&nbsp;is global minimum of&nbsp;\(f\)&nbsp;over&nbsp;\(S\)&nbsp;\(\iff\)\(\nabla f(\mathbf x^*)^T(\mathbf x - \mathbf x^*)\geq 0, \; \; \mathbf x \in S\).<br><br><b>Proof</b><br>""\(\implies \)"" part<br>Let&nbsp;\(\mathbf p\in \mathbb R^n\)&nbsp;and&nbsp;\(\alpha &gt; 0\).&nbsp;Taylor expanding towards&nbsp;\(\mathbf p\)&nbsp;with&nbsp;\(\alpha \)&nbsp;<br>\(f(\mathbf x^* + \alpha \mathbf p) = f(\mathbf x^*) + \alpha\nabla f(\mathbf x^*)^T \mathbf p + o(\alpha)\).<br>If&nbsp;&nbsp;\(\mathbf p\)&nbsp;is a descent direction from&nbsp;\(\mathbf x^*\)&nbsp;then&nbsp;\(\alpha\nabla f(\mathbf x^*)^T \mathbf p\lt 0\)&nbsp;implies that&nbsp;\(f(\mathbf x^* + \alpha \mathbf p) &lt; f(\mathbf x^*)\)&nbsp;for small enough&nbsp;\(\alpha\). <br>But given&nbsp;\(\mathbf x^*\)&nbsp;is a global minimum, there is no feasible descent direction&nbsp;\(\mathbf p\), therefore the only possibility is that&nbsp;\(\nabla f(\mathbf x^*)^T \mathbf p\geq 0\). Now letting&nbsp;\(\mathbf p = \mathbf x - \mathbf x^*, \;\; \mathbf x\in S\). The proof is done.<br>""\(\Longleftarrow\)"" part<br>The convexity of&nbsp;\(f\)&nbsp;yields that for every&nbsp;\(\mathbf x \in S\),<br>\(f(\mathbf x)\geq f(\mathbf x^*) + \nabla f(\mathbf x^*)^T(\mathbf x - \mathbf x^*) \geq f(\mathbf x^*)\). The result follows."	
<b>Thrm. 4.29,</b>&nbsp;The separation Theorem	"<b>Definition</b><br>Suppose that&nbsp;\(C \in \mathbb R^n\)&nbsp;is nonempty, closed and convex. Let&nbsp;\(\mathbf y\notin C\).<br>Then&nbsp;\(\exists \mathbf \pi \not= \mathbf 0^n,\alpha\in\mathbb R\)&nbsp;such that&nbsp;\(\mathbf \pi^T \mathbf y \gt \alpha\)&nbsp;and&nbsp;\(\mathbf \pi^T\mathbf x \leq \alpha\)&nbsp;&nbsp;\(\forall \mathbf x \in C\).<br><br><b>Proof</b><br>Define&nbsp;\(f : \mathbb R^n \to \mathbb R\)&nbsp;through&nbsp;\(f(\mathbf x) : =|| \mathbf x - \mathbf y||^2/2\),&nbsp;\(\mathbf x \in \mathbb R^n\). By Weierstrass Theorem there exists a minimum&nbsp;\(\mathbf x^*\)&nbsp;of&nbsp;\(f\)&nbsp;over&nbsp;\(C\)&nbsp;which is characterized by the variational inequality&nbsp;\((\mathbf y - \mathbf x^*)^T(\mathbf x - \mathbf x^*)\leq 0\)&nbsp;\(\forall \mathbf x \in C\)&nbsp;(this is true as&nbsp;\(-\nabla f(\mathbf x^*) = \mathbf y - \mathbf x^*\)).<br>Think of this as being the closest point to from the&nbsp;\(C\)&nbsp;to&nbsp;\(\mathbf y\).<br>Let&nbsp;\(\mathbf \pi := \mathbf y - \mathbf x^* \not = \mathbf 0\)&nbsp;as&nbsp;\(\mathbf y \notin C\). Let&nbsp;\(\alpha = (\mathbf y - \mathbf x^*)^T\mathbf x^*\)<br>Then&nbsp;\((\mathbf y - \mathbf x^*)^T(\mathbf x - \mathbf x^*)\leq 0\)&nbsp;yields that&nbsp;\(\mathbf \pi^T\mathbf x \leq \mathbf \pi^T \mathbf x^* = \alpha\)&nbsp;for all&nbsp;\(\mathbf x \in C\), while&nbsp;\(\pi^T\mathbf y - \alpha = (\mathbf y - \mathbf x^*)^T(\mathbf y - \mathbf x^*) = ||\mathbf y - \mathbf x^*||^2\gt 0\).<br><br>This proof can be understood easier geometrically, where the&nbsp;\(\mathbf \pi^T\mathbf x\)&nbsp;vector is the line seperation the set&nbsp;\(C\)&nbsp;with the point&nbsp;\(\mathbf y\).<br><img src=""paste-b41b8058e8853c22e6bd4acb913034e742b01444.jpg""><br>"	
<b>Thrm. 5.49,&nbsp;</b>Sufficiency of the KKT conditions for convex problems	"<b>Definition<br></b>If&nbsp;<br>\(\min f(\mathbf x)\)<br>\(s.t. g_i(\mathbf x)\leq 0, i = 1,\ldots,m\)<br>\(h_j(\mathbf x) = 0, j = 1,\ldots ,l\)<br>where&nbsp;\(f, g_i\)&nbsp;are&nbsp;\(C^1\),&nbsp;convex and&nbsp;\(h_j\)&nbsp;is affine,&nbsp;then the KKT conditions are a sufficient optimality condition.<br>Can also be stated as&nbsp;<br><br>\(\left.
\begin{array}{l}
 &amp;\text{The problem is convex} \\
    &amp;\text{KKT conditions hold at } \mathbf{x}^* 
\end{array}
\right\}\implies \\
    \mathbf{x}^* \text{ is a global minimum for the problem}\)<br><b><br>Proof</b><br>Assume the optimal point is&nbsp;\(\mathbf x^*\), then by convexity of&nbsp;\(g_i\),&nbsp;\(\forall \mathbf x\in S\),&nbsp;\(-\nabla g_i(\mathbf x^*)^T(\mathbf x - \mathbf x^*) \geq g_i(\mathbf x^*) - g_i(\mathbf x) = -g_i(\mathbf x) \geq 0\)&nbsp;for all&nbsp;\(i \in \mathcal I(\mathbf x^*)\).&nbsp;<br>Applying the affinity of all&nbsp;\(h_j\)&nbsp;functions,&nbsp;\(-\nabla h_j(\mathbf x^*)^T(\mathbf x - \mathbf x^*) =h_j(\mathbf x^*) - h_j(\mathbf x) = 0\).<br>Using the convexity of the obj. function, proper treatments of equality constraints together with non-negative Lagrange multipliers&nbsp;\(\mu_i\),&nbsp;\(i\in\mathcal I(\mathbf x^*)\), the following inequality is obtained<br>\(f(\mathbf x) -f(\mathbf x^*) \geq \nabla f(\mathbf x^*)^T(\mathbf x - \mathbf x^*) = -\sum_{i\in \mathcal I(\mathbf x^*)}\mu_i\nabla g_i(\mathbf x ^*)T(\mathbf x - \mathbf x^*) -\sum_{i\in \mathcal I(\mathbf x^*)}\lambda_i\nabla h_i(\mathbf x ^*)T(\mathbf x - \mathbf x^*) \geq 0\), the point&nbsp;\(\mathbf x\)&nbsp;was chosen arbitrary, whence&nbsp;\(\mathbf x^*\)&nbsp;solves the problem."	
<b>Thrm 6.1,</b>&nbsp;Relaxation Theorem	<b>Definition</b><br>Relax the problem&nbsp;<br>\(f^* = \text{infimum } f(\mathbf x)\)<br>\(s.t. \mathbf x\in S\)&nbsp;\(\to\)<br><br>\(f^*_R=\text{infimum } f_R(\mathbf x)\)<br>\(s.t. x\in S_R\).<br>With the property&nbsp;\(f_R\leq f\)&nbsp;on&nbsp;\(S\), and where&nbsp;\(S_R \supseteq S\). (feasible set larger, objective function smaller).<br><ol><li>\(f_R^*\leq f^*.\)<br></li><li>If the relaxed problem is infeasible, then so is the primal problem.</li><li>If the relaxed problem has an optimal solution&nbsp;\(\mathbf x^*_R \in S\)&nbsp;for which&nbsp;\(f_R(\mathbf x^*_R) = f(\mathbf x^*_R)\), then&nbsp;\(\mathbf x^*_R\)&nbsp;is also an optimal solution to the primal problem.</li></ol><div><b>Proof</b></div><div><ol><li>From definition.</li><li>From definition.</li><li>Given that the relaxed problem is always smaller than or equal to the original problem, if the same point works in both problems, then&nbsp;\(f_R(\mathbf x^*_R) = f(\mathbf x^*_R) \)&nbsp;implies that&nbsp;\(f_R(\mathbf x^*_R) = f(\mathbf x^*_R) \leq f_R(\mathbf x)\leq f(\mathbf x), \; \forall \mathbf x\in S\), meaning that&nbsp;\( f(\mathbf x^*_R)\)&nbsp;must be the optimal value for the original problem.</li></ol></div>	
<b>Thrm. 6.8,</b>&nbsp;Global optimality conditions in the abscence of a duality gap	<b>Definition<br></b>The vector&nbsp;\((\mathbf x^*, \hat\mu^*) \)&nbsp;is a pair of primal optimal solution and Lagrange multiplier&nbsp;\(\iff\)<br><ol><li>\(\hat\mu^*\geq \mathbf 0^m\)&nbsp;(Dual feasibility)<br></li><li>\(\mathbf x^* \in \arg \min_{x\in X}L(\mathbf x, \hat\mu^*)\)&nbsp;(Lagrangian optimality)</li><li>\(\mathbf g(\mathbf x^*)\leq \mathbf 0^m, \mathbf x\in X\)&nbsp;(Primal feasibility)</li><li>\(\mu^*_i g_i(\mathbf x^*) = 0\), (Complementary slackness)<br></li></ol><div><b>Proof</b></div><div>Suppose the pair satisfies above. then from 1 the Lagrangian problem to minimize&nbsp;\(L(\mathbf x, \mu^*)\)&nbsp;is a relaxation, where&nbsp;\(\mathbf x^*\)&nbsp;in&nbsp;2 solve this.</div><div>This is further solidified as 3 show feasibility and 4 implies that&nbsp;\(L(\mathbf x^*, \mu^*) = f(\mathbf x^*)\). This provides that&nbsp;\(\mathbf x^*\)&nbsp;is optimal in the original problem, implying that&nbsp;\(\hat\mu^*\)&nbsp;is a Lagrange multiplier vector.<br></div>	
<b>Thrm 6.5,&nbsp;</b>Weak Duality Theorem	<b>Definition</b><br>Let&nbsp;\(\mathbf x\)&nbsp;be feasible in&nbsp;<br>\(\inf_{\mathbf x}f(\mathbf x)\)<br>\( s.t.\; \mathbf x \in X,\)<br>\(g_i(\mathbf x)\leq 0, i =1,\ldots,m,\)<br>where&nbsp;\(f(\mathbf x):\mathbb R^n \to \mathbb R\)&nbsp;and&nbsp;\(g_i:\mathbb R^n\to \mathbb R\)&nbsp;are given functions, and&nbsp;\( X\in \mathbb R^n\).<br>Furthermore, let&nbsp;\(\boldsymbol \mu\)&nbsp;(a vector) be feasible in&nbsp;<br>\(\sup_\boldsymbol\mu q(\boldsymbol \mu)\)<br>\(s.t.\; \mu \geq \mathbf 0^m.\)&nbsp;<br>Where&nbsp;\(q(\boldsymbol\mu)\)&nbsp;is&nbsp;\(\inf_ \mathbf x\mathcal L(\mathbf x,\boldsymbol \mu) = \inf_{\mathbf x\in X}  (f(\mathbf x) +\boldsymbol \mu^T\mathbf g(\mathbf x))\).<br>Then it holds that&nbsp;\(q(\boldsymbol\mu) \leq f(\mathbf x)\), specifically&nbsp;\(q^* \leq f^*\).<br><br><b>Proof</b>&nbsp;<br>\(\forall \mu\geq \mathbf 0^m\)&nbsp;and&nbsp;\(\mathbf x \in X\)&nbsp;with&nbsp;\(\mathbf g(\mathbf x)\leq \mathbf 0^m\),<br>\(q(\boldsymbol\mu) = \inf_{\mathbf z\in X}\mathcal L(\mathbf z,\boldsymbol \mu)\leq f(\mathbf x) + \mu^T\mathbf g(\mathbf x)\leq f(\mathbf x)\).<br>Especially<br>\(q^* = \sup_{\boldsymbol\mu \geq \mathbf 0^m} q(\boldsymbol\mu)\leq \inf_{\mathbf x\in X: \mathbf g(\mathbf x)\leq 0^m} f(\mathbf x) = f^*\).\(\blacksquare\)	
<b>Thrm. 10.4,&nbsp;</b>Weak Duality Theorem (LP)	<b>Definition</b><br>If&nbsp;\(\mathbf x\)&nbsp;is a feasible solution to&nbsp;<br>\(\min z = \mathbf c^T\mathbf x\)<br>\(s.t. \; A\mathbf x = \mathbf b\),<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\(\mathbf x\geq \mathbf 0^n\),<br>where&nbsp;\(A\in \mathbb R^{m\times n}, \mathbf b\in \mathbb R^{m}\)&nbsp;and&nbsp;\(\mathbf c\in \mathbb R^{n}\).<br><br>And if&nbsp;\(\mathbf y\)&nbsp;is a feasible solution to<br>\(\max w = \mathbf b^T\mathbf y\),<br>\(s.t.\; A^T\mathbf y \leq \mathbf c\),<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\(\mathbf y \text{ free}\).<br>Then&nbsp;\(\mathbf c^T\mathbf x \geq \mathbf b^T\mathbf y\).<br><b>Proof<br></b>\(\mathbf c^T \mathbf x\geq (A^T\mathbf y)^T\mathbf x, \;\;\;\;\;\;\;\; [\mathbf c \geq A^T\mathbf y, \;\; \mathbf x\geq \mathbf 0^n]\)<br>\(\;\;\;\;\;\;\; = \mathbf y^T A\mathbf x = \mathbf y^T\mathbf b \;\;\; [A\mathbf x = \mathbf b]\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\)<br>\(\;\;\;\;\;\;\; =\mathbf b^T\mathbf y\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\)	
<b>Thrm 10.10,&nbsp;</b>Farka's Lemma (LP)	<b>Definition</b><br>Let&nbsp;\(A \in\mathbb R^{m\times n}\)&nbsp;and&nbsp;\(\mathbf b \in\mathbb R^{n}\). Then exactly one of the following systems has a feasible solution, and the other system is inconsistent.<br><br>(I)<br>\(A\mathbf x = \mathbf b,\)<br>\( x\geq \mathbf 0^n\)<br><br>(II)<br>\(A^T\mathbf y \leq \mathbf 0^n,\)<br>\(\mathbf b^T\mathbf y \gt 0\)<br><br><b>Proof<br></b>If (I) has a solution&nbsp;\(\mathbf x\)&nbsp;then&nbsp;\(\mathbf b^T\mathbf y = \mathbf x^TA^T\mathbf y \gt 0\), but&nbsp;\(\mathbf x \geq \mathbf 0^n\)&nbsp;so&nbsp;\(\mathbf x^T A^T \mathbf y\leq \mathbf 0^n\)&nbsp;can not hold, meaning that (II) is infeasible.<br>Assume (II) is infeasible. Consider the linear program<br>\(\max \;\;\mathbf b^T\mathbf y\),<br>\(s.t. \;\; A^T\mathbf y\leq \mathbf 0^n\),<br>\(\;\;\;\;\;\; y\;\; \text{   free}\),<br>and its dual program<br>\(\min \;\; (\mathbf 0^n)^T\mathbf x,\)<br>\(s.t. \;\;\;\; A\mathbf x = \mathbf b, \)<br>\(\;\;\;\;\;\;\;\;\;\;\; \mathbf x \geq \mathbf 0^n\).<br>Since (II) is infeasible,&nbsp;\(\mathbf y =\mathbf 0^m\)&nbsp;is an optimal solution to the primal linear program. Hence, the Strong Duality Theorem implies that there exists an optimal solution to the dual linear program. This solution is feasible in (I).<br>This is equivalent to the equivalence&nbsp;<br>\((\text{I}) \iff \neg (\text{II})\)<br>\(\iff\)<br>\(\neg (\text{I}) \iff (\text{II})\).<br>Establishing that precisely one of the two systems has a solution.&nbsp;&nbsp;	
<b>Thrm 10.6</b>, Strong Duality Theorem (LP)	<b>Definition</b><br>If the primal P and the dual D have feasible solutions, then there exist optimal solutions to P and D, and their optimal objective values are equal.<br><b>&nbsp;<br>Proof<br></b>Since the dual D is feasible it follows from the Weak Duality Theorem 10.4 that the objective function value of P is bounded from below. Hence Theorem 8.10 implies that there exists an optimal BFS,&nbsp;\(\mathbf x^* = \left(\mathbf x^T_B , \mathbf x^T_N\right)^T\)&nbsp;to P. Creating the optimal solution to D<br>\(\left(\mathbf y^* \right)^T = \mathbf c^T_BB^{-1}\).<br>Since&nbsp;\(\mathbf x ^*\)&nbsp;is an optimal basic feasible solution the reduced costs of the non-basic variables are non-negative, which provide that&nbsp;<br>\(A^T\mathbf y^* \leq \mathbf c\).<br>Showing that&nbsp;\(\mathbf y^*\)&nbsp;is feasible to D. Further, it is true that<br>\(\mathbf b^T\mathbf y^* = \mathbf b^T(B^{-1})^T\mathbf c_B = \mathbf c_B^TB^{-1}\mathbf b = \mathbf c^T_B\mathbf x_B = \mathbf c^T\mathbf x^*\)<br>providing that&nbsp;\(\mathbf y^*\)&nbsp;is also the optimal solution to D.	
<b>Thrm 9.11,&nbsp;</b>Finiteness of Simplex Algorithm	<b>Definition</b><br>If all of the basic feasible solutions are non-degenerate, then the simplex algorithm terminates after a finite number of iterations.<br><br><b>Proof</b><br>If a basic feasible solution is non-degenerate, then it follows that is has exactly&nbsp;\(m\)&nbsp;positive components, and hence has a unique associated basis. In this case, in the minimum ratio test<br>\(\mu^* = \text{minimum}_{i\in\{k |(B^{-1}N_j)k&gt;0\}}\frac{(B^{-1}\mathbf b)_i}{(B^{-1}N_j)i}\gt 0\).<br>therefore at each iteration, the objective value decreases and hence a basic feasible solution that has appeared once can never reappear. Further, given that the number of extreme points in a polyhedron is finite, that is BFSs, are finite, the proof is done.	
<b>Thrm 13.3,&nbsp;</b>Global Convergence of Penalty Method	"<b>Definition</b><br>Assume that&nbsp;<br>\(\min f(\mathbf x)\),<br>\(s.t. \;\; \mathbf x\in S\),<br>where&nbsp;\(S\subseteq \mathbb R^n\)&nbsp;is a nonempty, closed set and that&nbsp;\(f:\mathbb R^n \to \mathbb R\)&nbsp;is a given differentiable function, has optimal solutions.<br>Then every limit point of the sequence&nbsp;\(\{\mathbf x^*_\nu\},\nu \to +\infty\), of globally optimal solutions to the relaxed problem<br>\(\min _{\mathbf x\in \mathbb R^n} f(\mathbf x) + \nu\hat\chi_S(\mathbf x)\), where<br>\(\chi_S(\mathbf x)   =\begin{cases}
    0, &amp; \text{if } \mathbf x\in S\\
    +\infty, &amp; \text{otherwise}
  \end{cases}\)<br>and&nbsp;\(\nu\hat \chi_S(\mathbf x)\),&nbsp;\(\nu&gt; 0\)&nbsp;denotes as the&nbsp;<i>approximate indicator function</i>, is globally optimal to the first problem. That is,&nbsp;<br><br>\(\left. \begin{array}{ll}\mathbf x_\nu^* \;\;\text{ globally optimal in relaxation} \\ \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\mathbf x_\nu^* \to \mathbf x^* \;\;\text{as}\;\; \nu \to +\infty\end{array}\right\} \;\; \mathbf x^* \text{globally optimal in unrelaxed problem.}\)<br><b><br>Proof<br></b>Let&nbsp;\(\mathbf x^*\)&nbsp;denote an arbitrary globally optimal solution to the unrelaxed problem. From the inequality of the relaxation theorem<br>\(f(\mathbf x_\nu^*) + \nu\hat\chi_S(\mathbf x^*)\leq f(\mathbf x^*)\)<br>and that a penalization constitutes a relaxation,&nbsp;a uniform bound on the penalty term&nbsp;\(\nu\hat\chi_S(\mathbf x_\nu^*)\)&nbsp;for all&nbsp;\(\nu \geq 1\)&nbsp;is given as&nbsp;\(0\leq \nu\hat\chi_S(\mathbf x^*_\nu)\leq f(\mathbf x^*)- (\mathbf x_\nu^*)\).<br><br>Thus&nbsp;\(\hat \chi_S(\mathbf x_\nu^*)\)&nbsp;converges to zero as&nbsp;\(\nu \to +\infty\), and owing to the continuity of&nbsp;\(\hat \chi_S\)&nbsp;every limit point of the sequence&nbsp;\(\{\mathbf x^*_\nu\}\)&nbsp;must be feasible in the unrelaxed problem.<br>Let&nbsp;\(\mathbf{\hat x}\)&nbsp;denote an arbitrary limit point of said sequence, that is&nbsp;\(\lim_{k \to \infty}\mathbf x^*_{\nu_k} = \mathbf {\hat x}\)&nbsp;for some sequence&nbsp;\(\{\nu_k\} \)&nbsp;converging to infinity. Then the following inequalities hold<br>\(f(\hat{\mathbf x}) = \lim_{k \to \infty} f(\mathbf x^*_{\nu_k})\leq \lim_{k \to \infty}\{f(\mathbf x^*_{\nu_k}) + \nu_k\hat\chi_S(\mathbf x^*_{\nu_k})\} \leq f(\mathbf x^*)\)<br>where the last inequality follows from&nbsp;\(f(\mathbf x_\nu^*) + \nu\hat\chi_S(\mathbf x^*)\leq f(\mathbf x^*)\).<br>However, given the feasibility of&nbsp;\(\mathbf {\hat x}\)&nbsp;in the unrelaxed problem, the reverse inequality&nbsp;\(f(\mathbf x^*) \leq f(\mathbf{\hat x})\)&nbsp;also holds, providing&nbsp;\(f(\mathbf x^*) = f(\mathbf{\hat x})\).<br>"	
<b>Thrm. 10.11,&nbsp;</b>Complementary Slackness Theorem	"<b>Definition</b><br>Let&nbsp;\(\mathbf x\)&nbsp;be a feasible solution to the primal <br>\(\min z = \mathbf c^T\mathbf x\)<br>\(s.t. \;\; A\mathbf x=\mathbf b,\)<br>\(\mathbf x \geq \mathbf 0^n,\)<br><br>and&nbsp;\(\mathbf y\)&nbsp;be a feasible solution to the dual<br>\(\max w= \mathbf b^T\mathbf y,\)<br>\(s.t. \;\; A^T\mathbf y \leq \mathbf c\),<br>\(y \;\; \text{free}\).<br>Then<br>\(\left. \begin{array}{ll}
\mathbf x \; \text{optimal to P}\\
\mathbf y \; \text{optimal to D}
\end{array}\right\}\iff\)\(x_j(c_j -  A^T_j\mathbf y) = 0, \;\; j=1,\ldots, n\),<br>where&nbsp;\(A^T_j \)&nbsp;is the&nbsp;\(j^{\text{th}}\)&nbsp;column of&nbsp;\(A\).<br><br><b>Proof<br></b>If&nbsp;\(\mathbf x\)&nbsp;and&nbsp;\(\mathbf y\)&nbsp;are feasible it holds that&nbsp;<br>\(\mathbf c^T\mathbf x \geq (A^T\mathbf y)^T\mathbf x = \mathbf y^TA\mathbf x = \mathbf b^T \mathbf y\).<br>By the Strong Duality Theorem (LP) and the Weak Duality Theorem (LP),&nbsp;\(\mathbf x\)&nbsp;and&nbsp;\(\mathbf y\)&nbsp;are optimal if and only if&nbsp;\(\mathbf c^T \mathbf x = \mathbf b^T \mathbf y\), meaning that above is actually equality. That is&nbsp;<br>\(\mathbf c^T \mathbf x = (A^T\mathbf y)^T\mathbf x \iff \mathbf x^T(\mathbf c - A^T\mathbf y) = 0\)<br>since&nbsp;\(\mathbf x \geq \mathbf 0^n \)&nbsp;and that&nbsp;\(A^T\mathbf y \leq \mathbf c, \mathbf x^T(\mathbf c - A^T\mathbf y) = 0\)&nbsp;is equivalent to each term in the sum being zero, that is, the definition holds."	
