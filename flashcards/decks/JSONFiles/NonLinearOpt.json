[
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 3.48 a,&nbsp;</b>Prove the characterization of convexity in&nbsp;\\(C^1\\)&nbsp;(ONLY FIRST PART NEEDED FOR EXAM)",
        "definition": "Definition:<br>Let&nbsp;\\(f\\in C^1\\)&nbsp;on an open convex set&nbsp;\\(S\\).<br>\\(f\\)&nbsp;is convex on&nbsp;\\(S \\iff f(\\mathbf{ x}) \\geq f(\\mathbf{\\bar x}) + \\nabla f(\\mathbf{\\bar x})^T(\\mathbf x - \\mathbf{\\bar x}), \\forall \\mathbf x,\\mathbf{\\bar x} \\in S.\\)<ol><li>\\(f\\)&nbsp;is convex on&nbsp;\\(S \\iff f(\\mathbf{ x}) \\geq f(\\mathbf{\\bar x}) + \\nabla f(\\mathbf{\\bar x})^T (\\mathbf x - \\mathbf{\\bar x}), \\forall \\mathbf x,\\mathbf{\\bar x} \\in S.\\)</li><li>\\(f\\)&nbsp;is convex on&nbsp;\\(S \\iff \\left[\\nabla f(\\mathbf{x}) -  \\nabla f(\\mathbf{\\bar x})\\right]^T(\\mathbf{ x} -\\mathbf {\\bar x} )  \\geq 0,\\forall \\mathbf x, \\mathbf{\\bar x} \\in S.\\)</li></ol>To prove the statements both 1 and 2 need their respective implication proved&nbsp;\\(\\Longleftarrow\\)&nbsp;and&nbsp;\\(\\Longrightarrow\\).<br>1.<br>\\('\\Longrightarrow' \\text{part}:\\)<br>Assume left side,&nbsp;\\(f\\)&nbsp;is convex on&nbsp;\\(S\\).<br>Let&nbsp;\\(\\mathbf x^1, \\mathbf x^2\\in S, \\lambda \\in (0,1)\\).<br>By convexity&nbsp;\\(f(\\lambda\\mathbf x^1 + (1-\\lambda)\\mathbf x^2) \\leq \\lambda f(\\mathbf x^1) + (1-\\lambda)f(\\mathbf x^2)\\)<br>\\(f(\\mathbf x^1) - f(\\mathbf x^2) \\geq \\frac{1}{\\lambda}\\left(f(\\lambda\\mathbf x^1 + (1-\\lambda)\\mathbf x^2) - f(\\mathbf x^2)\\right)\\)&nbsp;(this works because&nbsp;\\(\\frac{1-\\lambda}{\\lambda}f(\\mathbf x^2) =\\frac{1}{\\lambda}f(\\mathbf x^2) - \\frac{\\lambda}{\\lambda}f(\\mathbf x^2)\\))<br>Right hand side of the inequality tends to the directional derivative of&nbsp;\\(f\\)&nbsp;at&nbsp;\\(\\mathbf x^2\\)&nbsp;in the direction&nbsp;\\((\\mathbf x^1 - \\mathbf x^2) \\)&nbsp;when&nbsp;\\(\\lambda \\downarrow 0\\), so the limit becomes<br>\\(f(\\mathbf x^1) - f(\\mathbf x^2) \\geq \\nabla f(\\mathbf x^2)(\\mathbf x^1-\\mathbf x^2) \\implies\\)\\(f(\\mathbf x) \\geq f(\\mathbf {\\bar x}) + \\nabla f(\\mathbf{\\bar x})^T(\\mathbf x -\\mathbf {\\bar x} )\\)<br>\\('\\Longleftarrow' \\text{part}:\\)<br>We have that<br>\\(f(\\mathbf x^1) \\geq f(\\lambda\\mathbf x^1 + (1-\\lambda)\\mathbf x^2) + (1-\\lambda)\\nabla f(\\lambda \\mathbf x^1 + (1-\\lambda)\\mathbf x^2)^T(\\mathbf x^1 -\\mathbf x^2)\\),<br>\\(f(\\mathbf x^2) \\geq f(\\lambda\\mathbf x^1 + (1-\\lambda)\\mathbf x^2) + \\lambda\\nabla f(\\lambda \\mathbf x^1 + (1-\\lambda)\\mathbf x^2)^T(\\mathbf x^2 -\\mathbf x^1)\\).<br>Respectively multiply both inequalities by&nbsp;\\(\\lambda\\)&nbsp; and&nbsp;\\((1-\\lambda)\\)&nbsp;to provide the sought result.<br><br>2.<br>\\('\\Longrightarrow' \\text{part}:\\)<br>Using 1 and the two inequalities:<br>\\(f(\\mathbf{\\bar x}) \\geq f(\\mathbf{ x}) + \\nabla f(\\mathbf{x})^T(\\mathbf{\\bar x} - \\mathbf{x}), \\mathbf{\\bar x}, \\mathbf{ x}\\in S\\),<br>\\(f(\\mathbf{ x}) \\geq f(\\mathbf{ \\bar x}) + \\nabla f(\\mathbf{\\bar x})^T(\\mathbf{ x} - \\mathbf{\\bar x}), \\mathbf{\\bar x}, \\mathbf{ x}\\in S\\),<br>Added together yields&nbsp;<br>\\(\\left[ \\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{\\bar x})  \\right](\\mathbf{x} - \\mathbf{\\bar x}) \\geq 0, \\forall \\mathbf{\\bar x}, \\mathbf{x} \\in S.\\)<br><br>\\('\\Longleftarrow' \\text{part}:\\)<br>The MVT. states that<br>\\(f(\\mathbf x^2) - f(\\mathbf x^1) = \\nabla f(\\mathbf x)^T(\\mathbf x^2 - \\mathbf x^1)\\), where&nbsp;\\(\\mathbf x = \\lambda \\mathbf x^1 + (1-\\lambda)\\mathbf x^2, \\lambda \\in (0,1)\\).<br>By assumption&nbsp;\\(\\left[ \\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{ x}^1)  \\right](\\mathbf{x} - \\mathbf{ x}^1) \\geq 0\\), so&nbsp;\\((1- \\lambda)\\left[ \\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{x}^1)  \\right](\\mathbf{x^2} - \\mathbf{x}^1) \\geq 0\\).<br>From this follows that&nbsp;\\(\\nabla f(\\mathbf{x})^T(\\mathbf x^2 -\\mathbf x^1)\\geq \\nabla f(\\mathbf x ^1)^T(\\mathbf x ^2 - \\mathbf x ^1).\\)<br>By using the previous inequality together with the mean value theorem we get&nbsp;\\(f(\\mathbf x ^2)\\geq f(\\mathbf x ^1) + \\nabla f(\\mathbf x ^1)^T(\\mathbf x ^2 - \\mathbf x ^1)\\)&nbsp;\\(\\blacksquare\\)",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 4.3,&nbsp;</b>Fundamental Theorem of global optimality",
        "definition": "<b>Definition</b><br>The minimization problem&nbsp;<br>\\(\\min f(x), \\text{ subject to } x\\in S\\)&nbsp;where&nbsp;\\(S \\subseteq \\mathbb R^n\\not \\;\\;\\emptyset\\),<br>\\(f: \\mathbb R^n \\to \\mathbb R\\cup\\{+\\infty\\}\\)&nbsp;where&nbsp;\\(S\\)&nbsp;is convex and&nbsp;\\(f\\)&nbsp;is a convex function on&nbsp;\\(S\\)&nbsp;implies that all local minimum of&nbsp;\\(f\\)&nbsp;over&nbsp;\\(S\\)&nbsp;is also a global minimum.<br><br><b>Proof</b><br>Assume&nbsp;\\(x^*\\in S\\)&nbsp;is a local min but not a global min.&nbsp;<br>let&nbsp;\\(\\bar x\\in S\\)&nbsp;be a point s.t.&nbsp;\\(f(\\bar x) \\lt f(x^*).\\)<br>For any&nbsp;\\(\\lambda\\in(0,1),\\)&nbsp;\\(\\lambda(\\bar x) + (1-\\lambda)(x^*)\\in S\\)&nbsp;by the definition of convexity.<br>Since&nbsp;\\(f\\)&nbsp;is convex<br>\\(f\\bigr(\\lambda(\\bar x) + (1-\\lambda)(x^*)\\bigr)\\in S \\leq \\lambda f(\\bar x) + (1-\\lambda)f(x^*)\\in S \\implies\\)<br>\\(f\\bigr(\\lambda(\\bar x) + (1-\\lambda)(x^*)\\bigr)\\in S \\lt \\lambda f(x^*) + (1-\\lambda)f(x^*)\\in S \\)<br>If&nbsp;\\(\\lambda \\downarrow 0\\)&nbsp;left hand side goes to&nbsp;\\(f(x^*),\\)&nbsp;implying that&nbsp;\\(f(x^*) &gt; f(x^*)\\), reductio ad absurdum&nbsp;\\(\\square\\)",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 4.22,&nbsp;</b>Necessary optimality conditions,&nbsp;\\(C^1\\)&nbsp;case",
        "definition": "<b>Definition</b><br>Suppose&nbsp;\\(S \\subseteq \\mathbb R^n\\)&nbsp;and&nbsp;\\(f: \\mathbb R^n \\to \\mathbb R\\cup\\{+\\infty\\}\\)&nbsp;is in&nbsp;\\(C^1\\)&nbsp;around&nbsp;\\(\\mathbf x\\in S\\)&nbsp;for&nbsp;\\(f(\\mathbf x)\\lt +\\infty\\). Prove that<br><ol><li>If&nbsp;\\(\\mathbf x^* \\in S\\)&nbsp;is a local minimum of&nbsp;\\(f\\)&nbsp;over&nbsp;\\(S\\)&nbsp;then&nbsp;\\(\\nabla f(\\mathbf x^*)^T\\mathbf p \\geq 0\\)&nbsp;holds for every feasible direction&nbsp;\\(\\mathbf p\\)&nbsp;from&nbsp;\\(\\mathbf x^*\\).</li><li>Suppose that&nbsp;\\(S\\)&nbsp;is convex and that&nbsp;\\(f\\)&nbsp;is convex on&nbsp;\\(S\\)&nbsp;in&nbsp;\\(C^1\\), then if&nbsp;\\(\\mathbf x^*\\in S\\)&nbsp;is a local minimum of&nbsp;\\(f\\)&nbsp;over&nbsp;\\(S\\)&nbsp;then&nbsp;\\(\\nabla f(\\mathbf x^*)^T(\\mathbf x - \\mathbf x^*)\\geq0,\\;\\forall\\mathbf x\\in S\\)&nbsp;holds.<br></li></ol><div><b>Proof</b></div><ol><li>Taylor expand around&nbsp;\\(\\mathbf x^*\\),&nbsp;\\(f(\\mathbf x^* + \\alpha \\mathbf p) = f(\\mathbf x^*) + \\alpha\\nabla f(\\mathbf x^*)^T\\mathbf p + o(a), a\\in(0,\\delta], \\delta\\gt 0.\\)<br>If there exists a direction&nbsp;\\(\\mathbf p\\)&nbsp;for which it holds that&nbsp;\\(\\nabla f(\\mathbf x^*)^T\\mathbf p\\lt 0\\), then&nbsp;\\(f(\\mathbf x^* + \\alpha \\mathbf p)\\lt f(\\mathbf x^*)\\)&nbsp;for small enough&nbsp;\\(\\alpha\\). If&nbsp;\\(\\mathbf p\\)&nbsp;is a feasible direction, then a contradiction is fulfilled.<br><br></li><li>If&nbsp;\\(S\\)&nbsp;is convex then for every&nbsp;\\(\\mathbf x\\in S, \\mathbf p := \\mathbf x - \\mathbf x^*\\)&nbsp;is a feasible direction.&nbsp;\\(\\nabla f(\\mathbf x^*)(\\mathbf x - \\mathbf x^*)\\geq 0\\)&nbsp;follows from proof 1.</li></ol>&nbsp;<br><div><br></div>",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 4.23,</b>&nbsp;Necessary and sufficient global optimality conditions",
        "definition": "\"<b>Definition</b><br>Suppose&nbsp;\\(S\\subseteq \\mathbb R^n\\not\\;\\;\\emptyset\\)&nbsp;and convex. Let&nbsp;\\(f:\\mathbb R^n \\to \\mathbb R\\)&nbsp;be convex and&nbsp;\\(C^1\\)&nbsp;on&nbsp;\\(S\\).&nbsp;<br>Prove that this implies that&nbsp;<br>\\(\\mathbf x^*\\)&nbsp;is global minimum of&nbsp;\\(f\\)&nbsp;over&nbsp;\\(S\\)&nbsp;\\(\\iff\\)\\(\\nabla f(\\mathbf x^*)^T(\\mathbf x - \\mathbf x^*)\\geq 0, \\; \\; \\mathbf x \\in S\\).<br><br><b>Proof</b><br>\"\"\\(\\implies \\)\"\" part<br>Let&nbsp;\\(\\mathbf p\\in \\mathbb R^n\\)&nbsp;and&nbsp;\\(\\alpha &gt; 0\\).&nbsp;Taylor expanding towards&nbsp;\\(\\mathbf p\\)&nbsp;with&nbsp;\\(\\alpha \\)&nbsp;<br>\\(f(\\mathbf x^* + \\alpha \\mathbf p) = f(\\mathbf x^*) + \\alpha\\nabla f(\\mathbf x^*)^T \\mathbf p + o(\\alpha)\\).<br>If&nbsp;&nbsp;\\(\\mathbf p\\)&nbsp;is a descent direction from&nbsp;\\(\\mathbf x^*\\)&nbsp;then&nbsp;\\(\\alpha\\nabla f(\\mathbf x^*)^T \\mathbf p\\lt 0\\)&nbsp;implies that&nbsp;\\(f(\\mathbf x^* + \\alpha \\mathbf p) &lt; f(\\mathbf x^*)\\)&nbsp;for small enough&nbsp;\\(\\alpha\\). <br>But given&nbsp;\\(\\mathbf x^*\\)&nbsp;is a global minimum, there is no feasible descent direction&nbsp;\\(\\mathbf p\\), therefore the only possibility is that&nbsp;\\(\\nabla f(\\mathbf x^*)^T \\mathbf p\\geq 0\\). Now letting&nbsp;\\(\\mathbf p = \\mathbf x - \\mathbf x^*, \\;\\; \\mathbf x\\in S\\). The proof is done.<br>\"\"\\(\\Longleftarrow\\)\"\" part<br>The convexity of&nbsp;\\(f\\)&nbsp;yields that for every&nbsp;\\(\\mathbf x \\in S\\),<br>\\(f(\\mathbf x)\\geq f(\\mathbf x^*) + \\nabla f(\\mathbf x^*)^T(\\mathbf x - \\mathbf x^*) \\geq f(\\mathbf x^*)\\). The result follows.\"",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 4.29,</b>&nbsp;The separation Theorem",
        "definition": "\"<b>Definition</b><br>Suppose that&nbsp;\\(C \\in \\mathbb R^n\\)&nbsp;is nonempty, closed and convex. Let&nbsp;\\(\\mathbf y\\notin C\\).<br>Then&nbsp;\\(\\exists \\mathbf \\pi \\not= \\mathbf 0^n,\\alpha\\in\\mathbb R\\)&nbsp;such that&nbsp;\\(\\mathbf \\pi^T \\mathbf y \\gt \\alpha\\)&nbsp;and&nbsp;\\(\\mathbf \\pi^T\\mathbf x \\leq \\alpha\\)&nbsp;&nbsp;\\(\\forall \\mathbf x \\in C\\).<br><br><b>Proof</b><br>Define&nbsp;\\(f : \\mathbb R^n \\to \\mathbb R\\)&nbsp;through&nbsp;\\(f(\\mathbf x) : =|| \\mathbf x - \\mathbf y||^2/2\\),&nbsp;\\(\\mathbf x \\in \\mathbb R^n\\). By Weierstrass Theorem there exists a minimum&nbsp;\\(\\mathbf x^*\\)&nbsp;of&nbsp;\\(f\\)&nbsp;over&nbsp;\\(C\\)&nbsp;which is characterized by the variational inequality&nbsp;\\((\\mathbf y - \\mathbf x^*)^T(\\mathbf x - \\mathbf x^*)\\leq 0\\)&nbsp;\\(\\forall \\mathbf x \\in C\\)&nbsp;(this is true as&nbsp;\\(-\\nabla f(\\mathbf x^*) = \\mathbf y - \\mathbf x^*\\)).<br>Think of this as being the closest point to from the&nbsp;\\(C\\)&nbsp;to&nbsp;\\(\\mathbf y\\).<br>Let&nbsp;\\(\\mathbf \\pi := \\mathbf y - \\mathbf x^* \\not = \\mathbf 0\\)&nbsp;as&nbsp;\\(\\mathbf y \\notin C\\). Let&nbsp;\\(\\alpha = (\\mathbf y - \\mathbf x^*)^T\\mathbf x^*\\)<br>Then&nbsp;\\((\\mathbf y - \\mathbf x^*)^T(\\mathbf x - \\mathbf x^*)\\leq 0\\)&nbsp;yields that&nbsp;\\(\\mathbf \\pi^T\\mathbf x \\leq \\mathbf \\pi^T \\mathbf x^* = \\alpha\\)&nbsp;for all&nbsp;\\(\\mathbf x \\in C\\), while&nbsp;\\(\\pi^T\\mathbf y - \\alpha = (\\mathbf y - \\mathbf x^*)^T(\\mathbf y - \\mathbf x^*) = ||\\mathbf y - \\mathbf x^*||^2\\gt 0\\).<br><br>This proof can be understood easier geometrically, where the&nbsp;\\(\\mathbf \\pi^T\\mathbf x\\)&nbsp;vector is the line seperation the set&nbsp;\\(C\\)&nbsp;with the point&nbsp;\\(\\mathbf y\\).<br><img src=\"\"paste-b41b8058e8853c22e6bd4acb913034e742b01444.jpg\"\"><br>\"",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm 6.1,</b>&nbsp;Relaxation Theorem",
        "definition": "<b>Definition</b><br>Relax the problem&nbsp;<br>\\(f^* = \\text{infimum } f(\\mathbf x)\\)<br>\\(s.t. \\mathbf x\\in S\\)&nbsp;\\(\\to\\)<br><br>\\(f^*_R=\\text{infimum } f_R(\\mathbf x)\\)<br>\\(s.t. x\\in S_R\\).<br>With the property&nbsp;\\(f_R\\leq f\\)&nbsp;on&nbsp;\\(S\\), and where&nbsp;\\(S_R \\supseteq S\\). (feasible set larger, objective function smaller).<br><ol><li>\\(f_R^*\\leq f^*.\\)<br></li><li>If the relaxed problem is infeasible, then so is the primal problem.</li><li>If the relaxed problem has an optimal solution&nbsp;\\(\\mathbf x^*_R \\in S\\)&nbsp;for which&nbsp;\\(f_R(\\mathbf x^*_R) = f(\\mathbf x^*_R)\\), then&nbsp;\\(\\mathbf x^*_R\\)&nbsp;is also an optimal solution to the primal problem.</li></ol><div><b>Proof</b></div><div><ol><li>From definition.</li><li>From definition.</li><li>Given that the relaxed problem is always smaller than or equal to the original problem, if the same point works in both problems, then&nbsp;\\(f_R(\\mathbf x^*_R) = f(\\mathbf x^*_R) \\)&nbsp;implies that&nbsp;\\(f_R(\\mathbf x^*_R) = f(\\mathbf x^*_R) \\leq f_R(\\mathbf x)\\leq f(\\mathbf x), \\; \\forall \\mathbf x\\in S\\), meaning that&nbsp;\\( f(\\mathbf x^*_R)\\)&nbsp;must be the optimal value for the original problem.</li></ol></div>",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm 6.5,&nbsp;</b>Weak Duality Theorem",
        "definition": "<b>Definition</b><br>Let&nbsp;\\(\\mathbf x\\)&nbsp;be feasible in&nbsp;<br>\\(\\inf_{\\mathbf x}f(\\mathbf x)\\)<br>\\( s.t.\\; \\mathbf x \\in X,\\)<br>\\(g_i(\\mathbf x)\\leq 0, i =1,\\ldots,m,\\)<br>where&nbsp;\\(f(\\mathbf x):\\mathbb R^n \\to \\mathbb R\\)&nbsp;and&nbsp;\\(g_i:\\mathbb R^n\\to \\mathbb R\\)&nbsp;are given functions, and&nbsp;\\( X\\in \\mathbb R^n\\).<br>Furthermore, let&nbsp;\\(\\boldsymbol \\mu\\)&nbsp;(a vector) be feasible in&nbsp;<br>\\(\\sup_\\boldsymbol\\mu q(\\boldsymbol \\mu)\\)<br>\\(s.t.\\; \\mu \\geq \\mathbf 0^m.\\)&nbsp;<br>Where&nbsp;\\(q(\\boldsymbol\\mu)\\)&nbsp;is&nbsp;\\(\\inf_ \\mathbf x\\mathcal L(\\mathbf x,\\boldsymbol \\mu) = \\inf_{\\mathbf x\\in X}  (f(\\mathbf x) +\\boldsymbol \\mu^T\\mathbf g(\\mathbf x))\\).<br>Then it holds that&nbsp;\\(q(\\boldsymbol\\mu) \\leq f(\\mathbf x)\\), specifically&nbsp;\\(q^* \\leq f^*\\).<br><br><b>Proof</b>&nbsp;<br>\\(\\forall \\mu\\geq \\mathbf 0^m\\)&nbsp;and&nbsp;\\(\\mathbf x \\in X\\)&nbsp;with&nbsp;\\(\\mathbf g(\\mathbf x)\\leq \\mathbf 0^m\\),<br>\\(q(\\boldsymbol\\mu) = \\inf_{\\mathbf z\\in X}\\mathcal L(\\mathbf z,\\boldsymbol \\mu)\\leq f(\\mathbf x) + \\mu^T\\mathbf g(\\mathbf x)\\leq f(\\mathbf x)\\).<br>Especially<br>\\(q^* = \\sup_{\\boldsymbol\\mu \\geq \\mathbf 0^m} q(\\boldsymbol\\mu)\\leq \\inf_{\\mathbf x\\in X: \\mathbf g(\\mathbf x)\\leq 0^m} f(\\mathbf x) = f^*\\).\\(\\blacksquare\\)",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 6.8,</b>&nbsp;Global optimality conditions in the abscence of a duality gap",
        "definition": "<b>Definition<br></b>The vector&nbsp;\\((\\mathbf x^*, \\hat\\mu^*) \\)&nbsp;is a pair of primal optimal solution and Lagrange multiplier&nbsp;\\(\\iff\\)<br><ol><li>\\(\\hat\\mu^*\\geq \\mathbf 0^m\\)&nbsp;(Dual feasibility)<br></li><li>\\(\\mathbf x^* \\in \\arg \\min_{x\\in X}L(\\mathbf x, \\hat\\mu^*)\\)&nbsp;(Lagrangian optimality)</li><li>\\(\\mathbf g(\\mathbf x^*)\\leq \\mathbf 0^m, \\mathbf x\\in X\\)&nbsp;(Primal feasibility)</li><li>\\(\\mu^*_i g_i(\\mathbf x^*) = 0\\), (Complementary slackness)<br></li></ol><div><b>Proof</b></div><div>Suppose the pair satisfies above. then from 1 the Lagrangian problem to minimize&nbsp;\\(L(\\mathbf x, \\mu^*)\\)&nbsp;is a relaxation, where&nbsp;\\(\\mathbf x^*\\)&nbsp;in&nbsp;2 solve this.</div><div>This is further solidified as 3 show feasibility and 4 implies that&nbsp;\\(L(\\mathbf x^*, \\mu^*) = f(\\mathbf x^*)\\). This provides that&nbsp;\\(\\mathbf x^*\\)&nbsp;is optimal in the original problem, implying that&nbsp;\\(\\hat\\mu^*\\)&nbsp;is a Lagrange multiplier vector.<br></div>",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 8.10&nbsp;</b>Existence and properties of optimal solutions (Linear Programming Models)",
        "definition": "<b>Definition<br></b>Let&nbsp;\\(V  \\)&nbsp;be the extreme points&nbsp;\\(V: =\\{\\mathbf v^1, \\ldots , \\mathbf v^k\\}\\)&nbsp;to&nbsp;the standard form polyhedron&nbsp;\\(P :=\\{\\mathbf x \\in \\mathbb R^n | A\\mathbf x = \\mathbf b, \\mathbf x \\geq \\mathbf 0 \\} \\),&nbsp;\\(D : = \\{\\mathbf d^1, \\ldots ,\\mathbf d^n\\}\\)&nbsp;be the extreme directions for&nbsp;\\(C : = \\{\\mathbf x\\in \\mathbb R^n| A\\mathbf x = \\mathbf 0, \\; \\mathbf x\\geq \\mathbf 0\\}\\). Consider the linear program&nbsp;<br>\\(\\min z = \\mathbf c^T\\mathbf x,\\)<br>\\(\\mathbf x \\in P\\).<br><br>Then it holds that<br><ol><li>The problem has a finite optimal solution if and only if&nbsp;\\(P\\)&nbsp;is nonempty and&nbsp;\\(z\\)&nbsp;is lower bounded on&nbsp;\\(P\\), that is,&nbsp;\\(P\\)&nbsp;is nonempty and&nbsp;\\(\\mathbf c^T\\mathbf d^j\\geq 0,\\;\\)\\(\\forall \\mathbf d^j\\in D\\).</li><li>If the problem has a finite optimal solution, then a optimal solution is found among the extreme points.</li></ol><div><b>Proof</b></div><ol><li>Let&nbsp;\\(\\mathbf x \\in P\\). The it follows that&nbsp;\\(\\mathbf x = \\sum_{i}^k\\alpha_i \\mathbf v^i + \\sum_{j}^r\\beta_j\\mathbf d^j\\),&nbsp;for some&nbsp;\\(\\alpha_1, \\ldots, \\alpha_k \\geq 0\\)&nbsp;s.t.&nbsp;\\(\\sum \\alpha =1\\)&nbsp;and&nbsp;\\(\\beta_1\\ldots\\beta_r \\geq 0\\). Hence the multiplication of&nbsp;\\(\\mathbf c^T\\)&nbsp;provides that&nbsp;\\(\\mathbf c^T\\mathbf x = \\sum_{i}^k\\alpha_i \\mathbf c^T\\mathbf v^i + \\sum_{j}^r\\beta_j \\mathbf c^T\\mathbf d^j \\). Given that&nbsp;\\(z = \\mathbf c^T \\mathbf x\\)&nbsp;it is clear it is dependent on the variations of the weights&nbsp;\\(\\alpha_i\\)&nbsp;and&nbsp;\\(\\beta_j\\), who are bounded by&nbsp;\\(\\sum \\alpha =1\\)&nbsp;and the fact that&nbsp;\\(\\beta_j\\mathbf d^j \\geq 0 \\;\\; \\forall \\mathbf d^j\\in D\\), since otherwise the corresponding&nbsp;\\(\\beta\\)&nbsp;to the inequality would tend to infinity as it provides that&nbsp;\\(z \\to -\\infty\\). <br>Given that&nbsp;\\(\\mathbf c^T\\mathbf d^j\\geq 0\\;\\forall j\\)&nbsp;the optimal value is to set&nbsp;\\(\\boldsymbol \\beta = \\mathbf 0\\), removing any addition to the minimization of&nbsp;\\(\\mathbf c^T\\mathbf x\\). This makes&nbsp;the problem only to minimize&nbsp;\\(\\mathbf c^T\\mathbf x = \\sum_{i}^k\\alpha_i \\mathbf c^T\\mathbf v^i\\), that is, in the convex hull of&nbsp;\\(P\\).<br><br></li><li>Assume&nbsp;\\(\\mathbf x\\in P\\)&nbsp;is an optimal solution and let&nbsp;\\(\\mathbf x\\)&nbsp;be expressed as the summation of&nbsp;\\(\\alpha \\)&nbsp;and&nbsp;\\(\\beta\\). Setting&nbsp;\\(\\forall \\beta = 0\\)&nbsp;again we have that&nbsp;\\(\\mathbf x= \\sum^k_i\\alpha_i\\mathbf v^i \\). <br>Further, let&nbsp;\\(a\\in \\arg \\min_{i\\in\\{1,\\ldots,k\\}}\\mathbf c^T\\mathbf v^i\\).Then it holds that&nbsp;<br>\\(\\mathbf c^T \\mathbf v^a = \\mathbf c^T\\mathbf v^a\\sum_i^k\\alpha_i =  \\sum_i^k\\alpha_i \\mathbf c^T\\mathbf v^a\\leq \\sum_i^k\\alpha_i\\mathbf c^T\\mathbf v^i = \\mathbf c^T\\mathbf x\\).<br>That is, the extreme point&nbsp;\\(\\mathbf v^a\\)&nbsp;is a global minimum.</li></ol>",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm 9.11,&nbsp;</b>Finiteness of Simplex Algorithm",
        "definition": "<b>Definition</b><br>If all of the basic feasible solutions are non-degenerate, then the simplex algorithm terminates after a finite number of iterations.<br><br><b>Proof</b><br>If a basic feasible solution is non-degenerate, then it follows that is has exactly&nbsp;\\(m\\)&nbsp;positive components, and hence has a unique associated basis. In this case, in the minimum ratio test<br>\\(\\mu^* = \\text{minimum}_{i\\in\\{k |(B^{-1}N_j)k&gt;0\\}}\\frac{(B^{-1}\\mathbf b)_i}{(B^{-1}N_j)i}\\gt 0\\).<br>therefore at each iteration, the objective value decreases and hence a basic feasible solution that has appeared once can never reappear. Further, given that the number of extreme points in a polyhedron is finite, that is BFSs, are finite, the proof is done.",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm. 10.4,&nbsp;</b>Weak Duality Theorem (LP)",
        "definition": "<b>Definition</b><br>If&nbsp;\\(\\mathbf x\\)&nbsp;is a feasible solution to&nbsp;<br>\\(\\min z = \\mathbf c^T\\mathbf x\\)<br>\\(s.t. \\; A\\mathbf x = \\mathbf b\\),<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\\(\\mathbf x\\geq \\mathbf 0^n\\),<br>where&nbsp;\\(A\\in \\mathbb R^{m\\times n}, \\mathbf b\\in \\mathbb R^{m}\\)&nbsp;and&nbsp;\\(\\mathbf c\\in \\mathbb R^{n}\\).<br><br>And if&nbsp;\\(\\mathbf y\\)&nbsp;is a feasible solution to<br>\\(\\max w = \\mathbf b^T\\mathbf y\\),<br>\\(s.t.\\; A^T\\mathbf y \\leq \\mathbf c\\),<br>&nbsp; &nbsp; &nbsp; &nbsp;&nbsp;\\(\\mathbf y \\text{ free}\\).<br>Then&nbsp;\\(\\mathbf c^T\\mathbf x \\geq \\mathbf b^T\\mathbf y\\).<br><b>Proof<br></b>\\(\\mathbf c^T \\mathbf x\\geq (A^T\\mathbf y)^T\\mathbf x, \\;\\;\\;\\;\\;\\;\\;\\; [\\mathbf c \\geq A^T\\mathbf y, \\;\\; \\mathbf x\\geq \\mathbf 0^n]\\)<br>\\(\\;\\;\\;\\;\\;\\;\\; = \\mathbf y^T A\\mathbf x = \\mathbf y^T\\mathbf b \\;\\;\\; [A\\mathbf x = \\mathbf b]\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\)<br>\\(\\;\\;\\;\\;\\;\\;\\; =\\mathbf b^T\\mathbf y\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\)",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm 10.10,&nbsp;</b>Farka's Lemma (LP)",
        "definition": "<b>Definition</b><br>Let&nbsp;\\(A \\in\\mathbb R^{m\\times n}\\)&nbsp;and&nbsp;\\(\\mathbf b \\in\\mathbb R^{n}\\). Then exactly one of the following systems has a feasible solution, and the other system is inconsistent.<br><br>(I)<br>\\(A\\mathbf x = \\mathbf b,\\)<br>\\( x\\geq \\mathbf 0^n\\)<br><br>(II)<br>\\(A^T\\mathbf y \\leq \\mathbf 0^n,\\)<br>\\(\\mathbf b^T\\mathbf y \\gt 0\\)<br><br><b>Proof<br></b>If (I) has a solution&nbsp;\\(\\mathbf x\\)&nbsp;then&nbsp;\\(\\mathbf b^T\\mathbf y = \\mathbf x^TA^T\\mathbf y \\gt 0\\), but&nbsp;\\(\\mathbf x \\geq \\mathbf 0^n\\)&nbsp;so&nbsp;\\(\\mathbf x^T A^T \\mathbf y\\leq \\mathbf 0^n\\)&nbsp;can not hold, meaning that (II) is infeasible.<br>Assume (II) is infeasible. Consider the linear program<br>\\(\\max \\;\\;\\mathbf b^T\\mathbf y\\),<br>\\(s.t. \\;\\; A^T\\mathbf y\\leq \\mathbf 0^n\\),<br>\\(\\;\\;\\;\\;\\;\\; y\\;\\; \\text{   free}\\),<br>and its dual program<br>\\(\\min \\;\\; (\\mathbf 0^n)^T\\mathbf x,\\)<br>\\(s.t. \\;\\;\\;\\; A\\mathbf x = \\mathbf b, \\)<br>\\(\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\mathbf x \\geq \\mathbf 0^n\\).<br>Since (II) is infeasible,&nbsp;\\(\\mathbf y =\\mathbf 0^m\\)&nbsp;is an optimal solution to the primal linear program. Hence, the Strong Duality Theorem implies that there exists an optimal solution to the dual linear program. This solution is feasible in (I).<br>This is equivalent to the equivalence&nbsp;<br>\\((\\text{I}) \\iff \\neg (\\text{II})\\)<br>\\(\\iff\\)<br>\\(\\neg (\\text{I}) \\iff (\\text{II})\\).<br>Establishing that precisely one of the two systems has a solution.&nbsp;&nbsp;",
        "tags": ""
    },
    {
        "category": "Mathematics::Nonlinear Optimization:: Theorems and Proofs",
        "term": "<b>Thrm 10.6</b>, Strong Duality Theorem (LP)",
        "definition": "<b>Definition</b><br>If the primal P and the dual D have feasible solutions, then there exist optimal solutions to P and D, and their optimal objective values are equal.<br><b>&nbsp;<br>Proof<br></b>Since the dual D is feasible it follows from the Weak Duality Theorem 10.4 that the objective function value of P is bounded from below. Hence Theorem 8.10 implies that there exists an optimal BFS,&nbsp;\\(\\mathbf x^* = \\left(\\mathbf x^T_B , \\mathbf x^T_N\\right)^T\\)&nbsp;to P. Creating the optimal solution to D<br>\\(\\left(\\mathbf y^* \\right)^T = \\mathbf c^T_BB^{-1}\\).<br>Since&nbsp;\\(\\mathbf x ^*\\)&nbsp;is an optimal basic feasible solution the reduced costs of the non-basic variables are non-negative, which provide that&nbsp;<br>\\(A^T\\mathbf y^* \\leq \\mathbf c\\).<br>Showing that&nbsp;\\(\\mathbf y^*\\)&nbsp;is feasible to D. Further, it is true that<br>\\(\\mathbf b^T\\mathbf y^* = \\mathbf b^T(B^{-1})^T\\mathbf c_B = \\mathbf c_B^TB^{-1}\\mathbf b = \\mathbf c^T_B\\mathbf x_B = \\mathbf c^T\\mathbf x^*\\)<br>providing that&nbsp;\\(\\mathbf y^*\\)&nbsp;is also the optimal solution to D.",
        "tags": ""
    }
]